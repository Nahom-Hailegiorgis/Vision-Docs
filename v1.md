// App.js
import React, { useState, useEffect, useRef } from "react";
import {
View,
Text,
TouchableOpacity,
StyleSheet,
ActivityIndicator,
} from "react-native";
import { CameraView, useCameraPermissions } from "expo-camera";
import { Audio } from "expo-av";
import { GOOGLE_VISION_API_KEY } from "@env";

const OBSTACLE_MAP = {
stairs: ["stairs", "staircase", "step"],
wall: ["wall", "door", "window", "partition"],
low: [
"shoe",
"bed",
"floor",
"box",
"backpack",
"bottle",
"cabinet",
"dresser",
"chest of drawers",
"rug",
"mat",
"robot vacuum",
"dog",
"cat",
"toy",
"bag",
"table",
],
head: [
"lamp",
"fan",
"ceiling fan",
"tv",
"doorframe",
"shelf",
"cabinetry",
"lighting",
"mirror",
"hanger",
"clothing",
"curtain rod",
"window blind",
],
ceiling: ["ceiling", "ceiling light", "light fixture", "chandelier"],
};

export default function App() {
const [hasPermission, setHasPermission] = useState(null);
const [loading, setLoading] = useState(false);
const cameraRef = useRef(null);
const [obstacleLabel, setObstacleLabel] = useState("");

// Map obstacle “types” to filenames and Audio.Sound instances
const SOUND_FILES = {
stairs: require("./assets/sounds/whistle.wav"),
wall: require("./assets/sounds/thud.wav"),
low: require("./assets/sounds/click.wav"),
head: require("./assets/sounds/beep.wav"),
ceiling: require("./assets/sounds/swoosh.wav"),
default: require("./assets/sounds/beep.wav"),
};
const soundMapRef = useRef({});

// Permission hook
const [permission, requestPermission] = useCameraPermissions();

useEffect(() => {
Audio.setAudioModeAsync({
allowsRecordingIOS: false,
staysActiveInBackground: false,
interruptionModeIOS: Audio.INTERRUPTION_MODE_IOS_DO_NOT_MIX,
playsInSilentModeIOS: true,
shouldDuckAndroid: false,
interruptionModeAndroid: Audio.INTERRUPTION_MODE_ANDROID_DO_NOT_MIX,
playThroughEarpieceAndroid: false,
});
}, []);

// 1) Request permission & preload all sounds
useEffect(() => {
if (permission === null) return;

    setHasPermission(permission.granted);
    if (!permission.granted) requestPermission();

    // Preload each sound into our map
    (async () => {
      const map = {};
      for (const [type, file] of Object.entries(SOUND_FILES)) {
        const sound = new Audio.Sound();
        await sound.loadAsync(file);
        map[type] = sound;
      }
      soundMapRef.current = map;
    })();

}, [permission]);

// 2) Classification logic
function classify(objs = [], labels = []) {
const objectNames = objs.map((obj) => obj.name.toLowerCase());
const labelNames = labels.map((label) => label.description.toLowerCase());

    // Combine both sets of names
    const allNames = [...objectNames, ...labelNames];

    for (const [type, keywords] of Object.entries(OBSTACLE_MAP)) {
      if (allNames.some((name) => keywords.some((kw) => name.includes(kw)))) {
        return type;
      }
    }

    return "default";

}

// 3) Detect & play
const detectObstacle = async () => {
if (!cameraRef.current) return;
setLoading(true);

    try {
      // snap + base64
      const photo = await cameraRef.current.takePictureAsync({
        quality: 0.7,
        skipMetadata: true,
        base64: true,
      });

      // ask for both localization & labels
      const body = {
        requests: [
          {
            image: { content: photo.base64 },
            features: [
              { type: "OBJECT_LOCALIZATION", maxResults: 5 },
              { type: "LABEL_DETECTION", maxResults: 10 },
            ],
          },
        ],
      };

      const res = await fetch(
        `https://vision.googleapis.com/v1/images:annotate?key=${GOOGLE_VISION_API_KEY}`,
        {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify(body),
        }
      );
      const json = await res.json();
      const resp = json.responses?.[0] || {};
      const objs = resp.localizedObjectAnnotations || [];
      const labels = resp.labelAnnotations || [];

      if (objs.length === 0) {
        // no object at all—no sound
        console.log("No obstacle detected");
        setObstacleLabel("");
      } else {
        // pick type based on labels
        const type = classify(objs, labels);

        setObstacleLabel(
          `${type.toUpperCase()}: ${objs.map((obj) => obj.name).join(", ")}`
        );

        console.log("== OBSTACLE DETECTED ==");
        console.log(
          "Localized Objects:",
          objs.map((obj) => obj.name)
        );
        console.log(
          "Label Annotations:",
          labels.map((label) => label.description)
        );
        console.log("Classified Type:", type);
        console.log(
          "Playing sound:",
          SOUND_FILES[type] ? `${type}.wav` : "default.wav"
        );

        const sound = soundMapRef.current[type] || soundMapRef.current.default;

        const boundingBox = objs[0].boundingPoly.normalizedVertices;
        let volume = 1.0;
        if (boundingBox.length >= 4) {
          const width = Math.abs(boundingBox[2].x - boundingBox[0].x);
          const height = Math.abs(boundingBox[2].y - boundingBox[0].y);
          const area = width * height;
          const rawVolume = Math.sqrt(area) * 1.5;
          volume = Math.min(Math.max(rawVolume, 0.1), 1.0);

          console.log("Object area:", area.toFixed(4));
          console.log("Volume based on distance:", volume.toFixed(2));
        }

        await sound.stopAsync(); // STOP any current playback
        await sound.setIsMutedAsync(false); // ENSURE sound is unmuted
        await sound.setVolumeAsync(volume); // SET volume BEFORE playing
        await sound.playAsync(); // PLAY fresh
      }
    } catch (err) {
      console.error("Detection error", err);
    } finally {
      setLoading(false);
    }

};

if (hasPermission === null) {
return (
<View style={styles.center}>
<Text>Requesting camera access…</Text>
</View>
);
}
if (hasPermission === false) {
return (
<View style={styles.center}>
<Text>No access to camera</Text>
</View>
);
}

return (
<View style={styles.container}>
{obstacleLabel ? (
<View style={styles.labelContainer}>
<Text style={styles.labelText}>{obstacleLabel}</Text>
</View>
) : null}

      <CameraView ref={cameraRef} style={styles.camera} cameraRatio="16:9" />
      <TouchableOpacity
        style={styles.button}
        onPress={detectObstacle}
        disabled={loading}
      >
        {loading ? (
          <ActivityIndicator color="#fff" />
        ) : (
          <Text style={styles.buttonText}>Scan for Obstacles</Text>
        )}
      </TouchableOpacity>
    </View>

);
}

const styles = StyleSheet.create({
container: { flex: 1, backgroundColor: "#000" },
camera: { flex: 1 },
button: {
position: "absolute",
bottom: 20,
alignSelf: "center",
paddingVertical: 14,
paddingHorizontal: 28,
backgroundColor: "#1e88e5",
borderRadius: 8,
},
buttonText: { color: "#fff", fontSize: 16 },
center: { flex: 1, justifyContent: "center", alignItems: "center" },
labelContainer: {
position: "absolute",
top: 50,
alignSelf: "center",
backgroundColor: "rgba(0,0,0,0.6)",
paddingHorizontal: 16,
paddingVertical: 8,
borderRadius: 6,
zIndex: 999,
},
labelText: {
color: "#fff",
fontSize: 20,
fontWeight: "bold",
},
});
